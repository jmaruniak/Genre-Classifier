{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import librosa.display\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import ReLU\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "# declare execution device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "    \n",
    "\n",
    "def load_data(data_path, test_size, validation_size, scale=False):\n",
    "    \"\"\"Loads training dataset from a .npy file.\n",
    "        :param data_path (str): Path to .npy file containing data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    data = np.load(data_path, allow_pickle=True).item()\n",
    "\n",
    "    # convert lists to tensors\n",
    "    X = torch.Tensor(data[\"spectrograms\"])\n",
    "    y = torch.Tensor(data[\"labels\"])\n",
    "    \n",
    "    if scale:\n",
    "        X, scale_min, scale_max = scale_input(X)\n",
    "\n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "    \n",
    "    # reshape data for an input layer\n",
    "    shape = np.shape(X_train)\n",
    "    X_train = X_train[..., np.newaxis].view(-1, 1, shape[1], shape[2])\n",
    "    X_validation = X_validation[..., np.newaxis].view(-1, 1, shape[1], shape[2])\n",
    "    X_test = X_test[..., np.newaxis].view(-1, 1, shape[1], shape[2])\n",
    "    \n",
    "    print(\"Data succesfully loaded!\")\n",
    "    \n",
    "    if scale:\n",
    "        return X_train, X_validation, X_test, y_train, y_validation, y_test, scale_min, scale_max\n",
    "    else:\n",
    "        return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "def scale_input(x, scale_min=None, scale_max=None):\n",
    "    if scale_min is None:\n",
    "        scale_min = x.min()\n",
    "    x = x - scale_min\n",
    "    if scale_max is None:\n",
    "        scale_max = x.max()\n",
    "    x = x / scale_max\n",
    "    x = x - 0.5\n",
    "    x = x * 2\n",
    "    \n",
    "    return x, scale_min, scale_max\n",
    "\n",
    "def unscale_input(x, scale_min, scale_max):\n",
    "    x = x / 2\n",
    "    x = x + 0.5\n",
    "    x = x * scale_max\n",
    "    x = x + scale_min\n",
    "    \n",
    "    return x\n",
    "\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=0)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=0)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(2, 2), padding=0)\n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(2, 2), padding=0)\n",
    "        self.norm4 = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 8, 128) # 32 * 8 -> shape after last convolution layer\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=1)\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=1)\n",
    "        self.act_func = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1st convolutional\n",
    "        x = self.act_func( self.conv1(x) )\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 2nd convolutional\n",
    "        x = self.act_func( self.conv2(x) )\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # 3rd convolutional\n",
    "        x = self.act_func( self.conv3(x) )\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.norm3(x)\n",
    "        \n",
    "        # 4th convolutional\n",
    "        x = self.act_func( self.conv4(x) )\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.norm4(x)\n",
    "        \n",
    "        # 1st linear\n",
    "        x = self.act_func( self.fc1(torch.flatten(x, 1, -1)) )\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 2nd linear = output layer\n",
    "        output = F.softmax( self.fc2(x) , dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def compile(self, model_name = None, model_state = None, optimizer_state = None):\n",
    "                    \n",
    "        self.to(device)\n",
    "        \n",
    "        if (model_name):\n",
    "            self.model_name = model_name\n",
    "        else:\n",
    "            self.model_name = f\"model-{int(time.time())}\"\n",
    "        \n",
    "        if (model_state):\n",
    "            self.load_state_dict(model_state)\n",
    "        \n",
    "        self.optimizer = optim.Adam([\n",
    "                    {'params': self.conv1.parameters(), 'weight_decay': 1e-4},\n",
    "                    {'params': self.norm1.parameters()},\n",
    "                    {'params': self.conv2.parameters(), 'weight_decay': 1e-4},\n",
    "                    {'params': self.norm2.parameters()},\n",
    "                    {'params': self.conv3.parameters(), 'weight_decay': 1e-3},\n",
    "                    {'params': self.norm3.parameters()},\n",
    "                    {'params': self.conv4.parameters(), 'weight_decay': 1e-3},\n",
    "                    {'params': self.norm4.parameters()},\n",
    "                    {'params': self.fc1.parameters(), 'weight_decay': 1e-4},\n",
    "                    {'params': self.fc2.parameters(), 'weight_decay': 1e-3}\n",
    "                ], lr=1e-3)\n",
    "        \n",
    "        if (optimizer_state):\n",
    "            self.optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, validation_data, epochs, batch_size, log=True, history=None):\n",
    "        \n",
    "        # declare history with default values before training\n",
    "        if history is None:\n",
    "            history = {'acc': [0.1], 'loss': [2.3026], 'val_acc': [0.1], 'val_loss': [2.3026]}\n",
    "        num_batches = math.ceil(len(X_train) / batch_size)\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        # iteration over epochs\n",
    "        for epoch in range(epochs):\n",
    "            # reset metrics after each epoch\n",
    "            if log:\n",
    "                running_loss = 0.0\n",
    "                running_acc = 0.0\n",
    "                self.train()\n",
    "            \n",
    "            # iteration over batches\n",
    "            for i in tqdm(range(0, len(X_train), batch_size)):\n",
    "                # batch data and load to device\n",
    "                X_batch = X_train[i:i+batch_size].to(device)\n",
    "                y_batch = y_train[i:i+batch_size].type(torch.LongTensor).to(device)\n",
    "\n",
    "                # forward and backward pass\n",
    "                self.zero_grad()\n",
    "                outputs = self(X_batch)\n",
    "                loss = self.loss_function(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # calculate running statistics\n",
    "                if log:\n",
    "                    matches = [torch.argmax(j)==k for j, k in zip(outputs, y_batch)]\n",
    "                    running_acc += matches.count(True)/len(matches)\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "            # add stats to history\n",
    "            if log:\n",
    "                running_acc /= num_batches\n",
    "                running_loss /= num_batches\n",
    "                running_val_acc, running_val_loss = self.test(validation_data[0], validation_data[1])\n",
    "                history['acc'].append(round(float(running_acc),4))\n",
    "                history['loss'].append(round(float(running_loss),4))\n",
    "                history['val_acc'].append(round(float(running_val_acc),4))\n",
    "                history['val_loss'].append(round(float(running_val_loss),4))            \n",
    "        \n",
    "        print(\"Training finished...\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "        \n",
    "    def test(self, X, y, batch_size=32, out=False):\n",
    "        \n",
    "        num_batches = math.ceil(len(X) / batch_size)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i in range(0, len(X), batch_size):\n",
    "            \n",
    "                X_batch = X[i:i+batch_size].to(device)\n",
    "                y_batch = y[i:i+batch_size].type(torch.LongTensor).to(device)\n",
    "                \n",
    "                outputs = self(X_batch)\n",
    "                loss = self.loss_function(outputs, y_batch)\n",
    "                \n",
    "                matches = [torch.argmax(i)==j for i, j in zip(outputs, y_batch)]\n",
    "                running_acc += matches.count(True)/len(matches)\n",
    "                running_loss += loss.item()\n",
    "                                \n",
    "            t_acc = round(float(running_acc / num_batches),4)\n",
    "            t_loss = round(float(running_loss / num_batches),4)\n",
    "            \n",
    "        if out:\n",
    "            print(f\"Test acc: {t_acc} Test loss: {t_loss}\")\n",
    "            \n",
    "        return t_acc, t_loss\n",
    "    \n",
    "    \n",
    "    def get_predictions(self, X, batch_size=32):\n",
    "    \n",
    "        all_outputs = torch.Tensor().to(device)\n",
    "        num_batches = math.ceil(len(X) / batch_size)\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i in range(0, len(X), batch_size):\n",
    "\n",
    "                X_batch = X[i:i+batch_size].to(device)\n",
    "                outputs = self(X_batch)\n",
    "                all_outputs = torch.cat((all_outputs, outputs), dim=0)\n",
    "\n",
    "        return all_outputs.cpu()\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    \n",
    "    path += \"/\" + model.model_name + \".pth\"\n",
    "    torch.save({\n",
    "            'name': model.model_name,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': model.optimizer.state_dict(),\n",
    "            }, path)\n",
    "    \n",
    "    \n",
    "def load_model(path):\n",
    "    \n",
    "    model = Model()\n",
    "    checkpoint = torch.load(path)\n",
    "    model.compile(checkpoint['name'], checkpoint['model'], checkpoint['optimizer'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def new_model():\n",
    "    \n",
    "    model = Model()\n",
    "    model.compile()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_confusion_matrix(X, y):\n",
    "    \n",
    "    stacked = torch.stack((y.type(torch.LongTensor), X.argmax(dim=1).type(torch.LongTensor)), dim=1)\n",
    "    matrix = torch.zeros(10,10).type(torch.LongTensor)\n",
    "    \n",
    "    for p in stacked:\n",
    "        target, predicted = p.tolist()\n",
    "        matrix[target, predicted] += 1\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    classes = [\"Blues\", \"Classical\", \"Country\", \"Disco\", \"Hip Hop\", \"Jazz\", \"Metal\", \"Pop\", \"Reggae\", \"Rock\"]\n",
    "    \n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, format(matrix[i, j], 'd'), ha=\"center\", va=\"center\", color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "def plot_history(history):\n",
    "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
    "        :param history: Training history of model\n",
    "        :return:\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    # create accuracy sublpot\n",
    "    axs[0].plot(history[\"acc\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history[\"val_acc\"], label=\"val accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"\")\n",
    "\n",
    "    # create error sublpot\n",
    "    axs[1].plot(history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history[\"val_loss\"], label=\"val error\")\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_spectrogram(Y, sr, hop_length, y_axis=\"linear\", size=None):\n",
    "    if size:\n",
    "        plt.figure(figsize=size)\n",
    "    librosa.display.specshow(Y, \n",
    "                             sr=sr, \n",
    "                             hop_length=hop_length, \n",
    "                             x_axis=\"time\", \n",
    "                             y_axis=y_axis)\n",
    "    plt.colorbar(format=\"%+2.f\")\n",
    "\n",
    "    \n",
    "class GuidedBackprop():\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \n",
    "        self.model = model\n",
    "        # variables for input gradients and forward activations\n",
    "        self.gradients = None\n",
    "        self.forward_relu_outputs = []\n",
    "        # update layers\n",
    "        self.update_relus()\n",
    "        self.hook_layers()\n",
    "    \n",
    "    \n",
    "    def hook_layers(self):\n",
    "        # setup hook to store final gradient on backward pass\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            self.gradients = grad_in[0]\n",
    "        \n",
    "        first_layer = list(self.model._modules.items())[0][1]\n",
    "        first_layer.register_backward_hook(hook_function)\n",
    "    \n",
    "    \n",
    "    def update_relus(self):\n",
    "        # set gradient to zero if negative or if corresponding ReLU activation is zero\n",
    "        def relu_backward_hook_function(module, grad_in, grad_out):\n",
    "            corresponding_forward_output = self.forward_relu_outputs[-1]\n",
    "            corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
    "            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n",
    "            del self.forward_relu_outputs[-1]\n",
    "            \n",
    "            return (modified_grad_out,)\n",
    "\n",
    "        # only store activations on forward pass\n",
    "        def relu_forward_hook_function(module, ten_in, ten_out):\n",
    "            self.forward_relu_outputs.append(ten_out)\n",
    "\n",
    "        # loop through layers, hook up ReLUs\n",
    "        for pos, module in self.model._modules.items():\n",
    "            \n",
    "            if isinstance(module, ReLU):\n",
    "                module.register_backward_hook(relu_backward_hook_function)\n",
    "                module.register_forward_hook(relu_forward_hook_function)\n",
    "\n",
    "    \n",
    "    def generate_gradients(self, input_sample, target_class):\n",
    "        \n",
    "        # compute gradients on input\n",
    "        input_sample.requires_grad = True\n",
    "        self.model.eval()\n",
    "        \n",
    "        # forward pass\n",
    "        model_output = self.model(input_sample)\n",
    "\n",
    "        # zero gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # create target gradient for backpropagation\n",
    "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
    "        one_hot_output[0][target_class] = 1\n",
    "\n",
    "        # backward pass\n",
    "        model_output.backward(gradient=one_hot_output)\n",
    "        \n",
    "        input_sample.requires_grad = False\n",
    "        gradients_as_arr = self.gradients.data.numpy()[0].squeeze()\n",
    "        \n",
    "        return gradients_as_arr\n",
    "    \n",
    "    \n",
    "def get_positive_negative_saliency(gradient):\n",
    "    \n",
    "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
    "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
    "    \n",
    "    return pos_saliency, neg_saliency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
