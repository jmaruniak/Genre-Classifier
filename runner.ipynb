{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import cnn_genre_classifier_spectrograms as mgr\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to file that stores spectrograms and genre labels for each processed segment\n",
    "# DATA_PATH = \"/data/shared/GTZAN-DATASET/spectrograms_10segments.npy\"\n",
    "\n",
    "# create new sets\n",
    "# X_train, X_validation, X_test, y_train, y_validation, y_test, scale_min, scale_max = mgr.load_data(DATA_PATH, test_size=0.20, validation_size=0.15, scale=True)\n",
    "\n",
    "#load existing sets\n",
    "dataset = torch.load(\"./data/shuffled_set_1_normalized.pth\")\n",
    "X_train = dataset['X_train']\n",
    "X_validation = dataset['X_validation']\n",
    "X_test = dataset['X_test']\n",
    "y_train = dataset['y_train']\n",
    "y_validation = dataset['y_validation']\n",
    "y_test = dataset['y_test']\n",
    "scale_min = dataset['scale_min']\n",
    "scale_max = dataset['scale_max']\n",
    "\n",
    "#save new data\n",
    "# torch.save({\n",
    "#         'X_train': X_train,\n",
    "#         'X_validation': X_validation,\n",
    "#         'X_test': X_test,\n",
    "#         'y_train': y_train,\n",
    "#         'y_validation': y_validation,\n",
    "#         'y_test': y_test,\n",
    "#         'scale_min': scale_min,\n",
    "#         'scale_max': scale_max,\n",
    "#         }, \"./data/spectro10_normalized.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "data =[\n",
    "        \"Blues\",\n",
    "        \"Classical\",\n",
    "        \"Country\",\n",
    "        \"Disco\",\n",
    "        \"Hiphop\",\n",
    "        \"Jazz\",\n",
    "        \"Metal\",\n",
    "        \"Pop\",\n",
    "        \"Reggae\",\n",
    "        \"Rock\"\n",
    "    ]\n",
    "\n",
    "bins = np.arange(0, y_validation.max().numpy() + 1.5) - 0.5\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "counts, _, patches = ax.hist((y_train.numpy(), y_validation.numpy(), y_test.numpy()), bins=bins, range=(), rwidth=0.75, label=('Train', 'Validation', 'Test'))\n",
    "\n",
    "for count, patch in zip(counts[0],patches[0]):\n",
    "    ax.annotate(str(int(count)), xy=(patch.get_x(), patch.get_height() + 5))\n",
    "    \n",
    "for count, patch in zip(counts[1],patches[1]):\n",
    "    ax.annotate(str(int(count)), xy=(patch.get_x(), patch.get_height() + 5))\n",
    "    \n",
    "for count, patch in zip(counts[2],patches[2]):\n",
    "    ax.annotate(str(int(count)), xy=(patch.get_x(), patch.get_height() + 5))\n",
    "    \n",
    "ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "ax.set_xticklabels(data, rotation=45, fontsize=12)\n",
    "plt.legend(loc=5, prop={'size': 12})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing or create new model\n",
    "\n",
    "# model = mgr.new_model()\n",
    "model = mgr.load_model(\"./saved_models/model-spectro10-epoch25-batch512-norm.pth\")\n",
    "\n",
    "print(summary(model, torch.rand(1, 1, np.shape(X_train)[2], np.shape(X_train)[3]).cuda()), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_validation, y_validation),\n",
    "    epochs=15,\n",
    "    batch_size=256,\n",
    "    log=True,\n",
    "    history=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "# history = torch.load(\"./saved_models/model-spectro10-epoch25-batch512-norm_history.pth\")\n",
    "mgr.plot_history(history)\n",
    "\n",
    "print(\"Max_train_acc:\", max(history['acc']), \"  Min_train_loss:\", min(history['loss']))\n",
    "print(\"Max_val_acc:\", max(history['val_acc']), \"  Min_val_loss:\", min(history['val_loss']))\n",
    "    \n",
    "t_acc, t_loss = model.test(X_test, y_test, out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename model\n",
    "\n",
    "model.model_name = 'model-spectro10-epoch25-batch512-norm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "mgr.save_model(model, \"./saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training history\n",
    "\n",
    "torch.save(history, \"./saved_models/\"  + model.model_name + \"_history.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and plot confusion matrix\n",
    "\n",
    "model_predictions = model.get_predictions(X_test)\n",
    "matrix = mgr.create_confusion_matrix(model_predictions, y_test)\n",
    "mgr.plot_confusion_matrix(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a music sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process specific musical sample, print spectrogram\n",
    "\n",
    "file_path = \"/data/shared/GTZAN-DATASET/genres_dataset/rock/rock.00030.wav\"\n",
    "segment = 5\n",
    "\n",
    "start = 66150 * segment\n",
    "finish = 66150 * (segment + 1)\n",
    "\n",
    "signal, sample_rate = librosa.load(file_path, sr=22050)\n",
    "S_signal = librosa.stft(signal[start:finish], n_fft=1024, hop_length=512)\n",
    "Y_signal = np.abs(S_signal) ** 2\n",
    "Y_log_signal = librosa.power_to_db(Y_signal)\n",
    "\n",
    "mgr.plot_spectrogram(Y_log_signal, sample_rate, 512, size=(20,10))\n",
    "\n",
    "ipd.Audio(signal[start:finish], rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for GBP and IG out of loaded sample\n",
    "\n",
    "model = mgr.load_model(\"./saved_models/model-spectro10-epoch25-batch512-norm.pth\").cpu().eval()\n",
    "sample_signal = mgr.scale_input(torch.Tensor(Y_log_signal), scale_min, scale_max)[0].unsqueeze(0).unsqueeze(0)\n",
    "output = model(sample_signal)\n",
    "sample_prediction = torch.argmax(output)\n",
    "print(output, sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruct original signal\n",
    "\n",
    "reconstruction = mgr.unscale_input(sample_signal[0][0], scale_min, scale_max)\n",
    "reconstruction = librosa.db_to_amplitude(reconstruction.numpy())\n",
    "y_inv = librosa.griffinlim(reconstruction, hop_length=512, win_length=1024)\n",
    "sf.write(\"./sample_original.wav\", y_inv, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate guided gradients and their pos/neg saliency maps\n",
    "\n",
    "model = mgr.load_model(\"./saved_models/model-spectro10-epoch25-batch512-norm.pth\").cpu().eval()\n",
    "gbp = mgr.GuidedBackprop(model)\n",
    "guided_grads = gbp.generate_gradients(sample_signal, sample_prediction)\n",
    "pos_sal_gbp, neg_sal_gbp = mgr.get_positive_negative_saliency(guided_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.plot_spectrogram(pos_sal_gbp, 22050, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct signal using guided gradients positive saliency as spectral mask\n",
    "\n",
    "reconstruction = mgr.unscale_input(sample_signal[0][0], scale_min, scale_max).numpy()\n",
    "reconstruction = librosa.db_to_amplitude(reconstruction)\n",
    "reconstruction_masked = reconstruction * pos_sal_gbp\n",
    "y_inv = librosa.griffinlim(reconstruction_masked, hop_length=512, win_length=1024)\n",
    "sf.write(\"./sample_masked_gbp.wav\", y_inv, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate integrated gradients and their pos/neg saliency maps\n",
    "\n",
    "model = mgr.load_model(\"./saved_models/model-spectro10-epoch25-batch512-norm.pth\").cpu().eval()\n",
    "baseline = torch.zeros(1, 1, 513, 130)\n",
    "ig = IntegratedGradients(model)\n",
    "integrated_grads = ig.attribute(sample_signal, baseline, sample_prediction).squeeze().numpy()\n",
    "pos_sal_ig, neg_sal_ig = mgr.get_positive_negative_saliency(integrated_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.plot_spectrogram(pos_sal_ig, 22050, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct signal using integrated gradients positive saliency as spectral mask\n",
    "\n",
    "reconstruction = mgr.unscale_input(sample_signal[0][0], scale_min, scale_max).numpy()\n",
    "reconstruction = librosa.db_to_amplitude(reconstruction)\n",
    "reconstruction_masked = reconstruction * pos_sal_ig\n",
    "y_inv = librosa.griffinlim(reconstruction_masked, hop_length=512, win_length=1024)\n",
    "sf.write(\"./sample_masked_ig.wav\", y_inv, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves images of attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def save_gradient_images(gradient, file_name):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "    # normalize between 0-1\n",
    "    gradient = gradient - gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    # save image\n",
    "    path_to_file = os.path.join('./results', file_name + '.jpg')\n",
    "    save_image(gradient, path_to_file)\n",
    "\n",
    "    \n",
    "def save_image(im, path):\n",
    "    if isinstance(im, (np.ndarray, np.generic)):\n",
    "        im = format_np_output(im)\n",
    "        im = Image.fromarray(im)\n",
    "    im.save(path)\n",
    "    \n",
    "\n",
    "def format_np_output(np_arr):\n",
    "    # repeat first channel and convert 1xWxH to 3xWxH\n",
    "    if np_arr.shape[0] == 1:\n",
    "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
    "    # convert to WxHx3 in order to make it saveable by PIL\n",
    "    if np_arr.shape[0] == 3:\n",
    "        np_arr = np_arr.transpose(1, 2, 0)\n",
    "    # multiply with 255 and change type to make it saveable by PIL\n",
    "    if np.max(np_arr) <= 1:\n",
    "        np_arr = (np_arr*255).astype(np.uint8)\n",
    "    return np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_gradient_images(attributions[0], 'sample' + '_ig')\n",
    "save_gradient_images(pos_sal_ig, 'sample' + '_pos_sal_ig')\n",
    "save_gradient_images(neg_sal_ig, 'sample' + '_neg_sal_ig')\n",
    "\n",
    "save_gradient_images(guided_grads, 'sample' + '_gbp')\n",
    "save_gradient_images(pos_sal_gbp, 'sample' + '_pos_sal_gbp')\n",
    "save_gradient_images(neg_sal_gbp, 'sample' + '_neg_sal_gbp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
